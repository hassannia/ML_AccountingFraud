{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6392e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile\n",
    "from extra_codes import calc_vif\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ML_Fraud:\n",
    "    __version__='1.0.5'\n",
    "    def __init__(self,sample_start=1991,test_sample=range(2001,2011),\n",
    "                 OOS_per=1,OOS_gap=0,sampling='expanding',adjust_serial=True,\n",
    "                 cv_type='kfold',temp_year=1,cv_flag=False,cv_k=10,write=True,IS_per=10):\n",
    "\n",
    "        if isfile('FraudDB2020.csv')==False:\n",
    "            df=pd.DataFrame()\n",
    "            for s in range(1,5):\n",
    "                fl_name='FraudDB2020_Part'+str(s)+'.csv'\n",
    "                new_df=pd.read_csv(fl_name)\n",
    "                df=df.append(new_df)\n",
    "            df.to_csv('FraudDB2020.csv',index=False)\n",
    "            \n",
    "        df=pd.read_csv('FraudDB2020.csv')\n",
    "        self.df=df\n",
    "        self.ss=sample_start\n",
    "        self.se=np.max(df.fyear)\n",
    "        self.ts=test_sample\n",
    "        self.cv_t=cv_type\n",
    "        self.cv=cv_flag\n",
    "        self.cv_k=cv_k\n",
    "        self.cv_t_y=temp_year\n",
    "        \n",
    "        sampling_set=['expanding','rolling']\n",
    "        if sampling in sampling_set:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Invalid sampling choice. Permitted options are \"expanding\" and \"rolling\"')\n",
    "        \n",
    "        self.sa=sampling\n",
    "        self.w=write\n",
    "        self.ip=IS_per\n",
    "        self.op=OOS_per\n",
    "        self.og=OOS_gap\n",
    "        self.a_s=adjust_serial\n",
    "        print('Module initiated successfully ...')\n",
    "        #The dir() function returns all properties and methods of the specified object, without the values.\n",
    "        list_methods=dir(self)\n",
    "        # .any: It checks for any element satisfying a condition and returns a True in case it finds any one element.\n",
    "        reduced_methods=[item+'()' for item in list_methods if any(['analy' in item,'compare' in item,item=='sumstats'])]\n",
    "        #string.join(iterable)\n",
    "        print('Procedures are: '+'; '.join(reduced_methods))\n",
    "    \n",
    "    def mc_analysis(self,B=1000,adjust_serial=None,C_FN=30,C_FP=1):\n",
    "       \n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from extra_codes import ndcg_k,relogit\n",
    "        from statsmodels.discrete.discrete_model import Logit\n",
    "        from statsmodels.tools import add_constant\n",
    "        \n",
    "        t0=datetime.now()\n",
    "        # setting the parameters\n",
    "        IS_period=self.ip\n",
    "        k_fold=self.cv_k\n",
    "        OOS_period=self.op # 1 year ahead prediction\n",
    "        OOS_gap=self.og # Gap between training and testing period\n",
    "        start_OOS_year=self.ts[0] #2001\n",
    "        end_OOS_year=self.ts[-1] #2010\n",
    "        sample_start=self.ss #1991\n",
    "        if adjust_serial==None:\n",
    "            adjust_serial=self.a_s\n",
    "        cross_val=self.cv\n",
    "        case_window=self.sa #expanding or rolling window\n",
    "        fraud_df=self.df\n",
    "        write=self.w\n",
    "        \n",
    "        \n",
    "        print('starting the MC analysis for case B='+str(B)+', serial treatment='+str(adjust_serial))\n",
    "        t000=datetime.now()\n",
    "        reduced_tbl_1=fraud_df.iloc[:,[0,1,3,7,8]]\n",
    "        reduced_tbl_2=fraud_df.iloc[:,9:-3]\n",
    "        reduced_tblset=[reduced_tbl_1,reduced_tbl_2]\n",
    "        reduced_tbl=pd.concat(reduced_tblset,axis=1)\n",
    "        reduced_tbl=reduced_tbl[reduced_tbl.fyear>=sample_start] #1991\n",
    "        reduced_tbl=reduced_tbl[reduced_tbl.fyear<=end_OOS_year] #2010\n",
    "\n",
    "        # Setting the cross-validation setting\n",
    "\n",
    "        tbl_year_IS_CV=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear<start_OOS_year,\\\n",
    "                                                   reduced_tbl.fyear>=start_OOS_year-IS_period)]\n",
    "        tbl_year_IS_CV=tbl_year_IS_CV.reset_index(drop=True)\n",
    "        \n",
    "        X_CV=tbl_year_IS_CV.iloc[:,-11:]\n",
    "        \n",
    "        Y_CV=tbl_year_IS_CV.AAER_DUMMY\n",
    "        \n",
    "        \n",
    "        X_CV_raw=tbl_year_IS_CV.iloc[:,5:-11]\n",
    "        \n",
    "        P_f=np.sum(Y_CV==1)/len(Y_CV)\n",
    "        P_nf=1-P_f\n",
    "\n",
    "        print('prior probablity of fraud between '+str(sample_start)+'-'+\n",
    "              str(start_OOS_year-1)+' is '+str(np.round(P_f*100,2))+'%')\n",
    "        \n",
    "        range_oos=range(start_OOS_year,end_OOS_year+1,OOS_period) #(2001,2010+1,1)\n",
    "\n",
    "\n",
    "        roc_set_ratio=np.zeros(len(range_oos))\n",
    "        sensitivity_ratio=np.zeros(len(range_oos))\n",
    "        specificity_ratio=np.zeros(len(range_oos))\n",
    "        precision_ratio=np.zeros(len(range_oos))\n",
    "        ndcg_ratio=np.zeros(len(range_oos))\n",
    "        ecm_ratio=np.zeros(len(range_oos))\n",
    "        \n",
    "        roc_set_ratio_raw=np.zeros(len(range_oos))\n",
    "        sensitivity_ratio_raw=np.zeros(len(range_oos))\n",
    "        specificity_ratio_raw=np.zeros(len(range_oos))\n",
    "        precision_ratio_raw=np.zeros(len(range_oos))\n",
    "        ndcg_ratio_raw=np.zeros(len(range_oos))\n",
    "        ecm_ratio_raw=np.zeros(len(range_oos))\n",
    "\n",
    "        m=0\n",
    "        \n",
    "        for yr in range_oos: #2001-2010\n",
    "            t1=datetime.now()\n",
    "            if case_window=='expanding':\n",
    "                year_start_IS=sample_start #1991\n",
    "            else:\n",
    "                year_start_IS=yr-IS_period #1991\n",
    "            #how many years between training and testing sample: \n",
    "            #expanding: 1991-2000, 1991-2001\n",
    "            #rolling: 1991-2000, 1992-2001\n",
    "            tbl_year_IS=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear<yr-OOS_gap,\\\n",
    "                                                       reduced_tbl.fyear>=year_start_IS)]\n",
    "            tbl_year_IS=tbl_year_IS.reset_index(drop=True)  \n",
    "            \n",
    "            misstate_firms=np.unique(tbl_year_IS.gvkey[tbl_year_IS.AAER_DUMMY==1])\n",
    "            #How many periods constitute the testing sample at a time: 2001, 2002\n",
    "            tbl_year_OOS=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear>=yr,\\\n",
    "                                                        reduced_tbl.fyear<yr+OOS_period)]\n",
    "            \n",
    "            if adjust_serial==True:\n",
    "                ok_index=np.zeros(tbl_year_OOS.shape[0])\n",
    "                for s in range(0,tbl_year_OOS.shape[0]):\n",
    "                    if not tbl_year_OOS.iloc[s,1] in misstate_firms:\n",
    "                        ok_index[s]=True\n",
    "                    \n",
    "                \n",
    "            else:\n",
    "                #filled with ones and keep all observations including serial frauds\n",
    "                ok_index=np.ones(tbl_year_OOS.shape[0]).astype(bool)\n",
    "                \n",
    "            #deleting observations where a company appears both in IS and OOS samples\n",
    "            tbl_year_OOS=tbl_year_OOS.iloc[ok_index==True,:]\n",
    "            tbl_year_OOS=tbl_year_OOS.reset_index(drop=True)  \n",
    "            \n",
    "            X_train_b=tbl_year_IS.iloc[:,-11:]\n",
    "            mean_vals=np.mean(X_train_b)\n",
    "            std_vals=np.std(X_train_b)\n",
    "            X_train_b=(X_train_b-mean_vals)/std_vals\n",
    "            Y_train_b=tbl_year_IS.AAER_DUMMY\n",
    "            \n",
    "            X_test_b=tbl_year_OOS.iloc[:,-11:]\n",
    "            X_test_b=(X_test_b-mean_vals)/std_vals\n",
    "            Y_test_b=tbl_year_OOS.AAER_DUMMY\n",
    "            \n",
    "            X_train_raw_b=tbl_year_IS.iloc[:,5:-11]\n",
    "            mean_vals_raw=np.mean(X_train_raw_b)\n",
    "            std_vals_raw=np.std(X_train_raw_b)\n",
    "            X_train_raw_b=(X_train_raw_b-mean_vals_raw)/std_vals_raw\n",
    "            \n",
    "            X_test_raw_b=tbl_year_OOS.iloc[:,5:-11]\n",
    "            X_test_raw_b=(X_test_raw_b-mean_vals_raw)/std_vals_raw\n",
    "            \n",
    "\n",
    "                \n",
    "            n_P=np.sum(Y_test_b==1)\n",
    "            n_N=np.sum(Y_test_b==0) # how many serial frauds we have dropped from the original positive fraud cases\n",
    "                \n",
    "            print('1')    \n",
    "            t01=datetime.now()\n",
    "            #Add a column of ones to an array.\n",
    "            clf_lr = Logit(Y_train_b,add_constant(X_train_b)).fit(disp=0)\n",
    "            predicted_test=clf_lr.predict(add_constant(X_test_b))\n",
    "            predicted_test=predicted_test.to_numpy()\n",
    "            roc_set_ratio[m]=roc_auc_score(Y_test_b,predicted_test)\n",
    "            #numpy.percentile()function used to compute the nth percentile of the given data (array elements) along the specified axis. \n",
    "            #classification threshold (99th percentile) = 1 or 0\n",
    "            cutoff_ratio=np.percentile(predicted_test,99)\n",
    "            # predicted value higher than the threshold value will flag the observation as positive, whether correctly or not (TP+FP)\n",
    "            labels_ratio=(predicted_test>=cutoff_ratio).astype(int)\n",
    "            #fraud correctly classified \n",
    "            sensitivity_ratio[m]=np.sum(np.logical_and(labels_ratio==1,Y_test_b==1))/np.sum(Y_test_b)\n",
    "            #non-fraud correctly classified\n",
    "            specificity_ratio[m]=np.sum(np.logical_and(labels_ratio==0,Y_test_b==0))/np.sum(Y_test_b==0)\n",
    "            #the number of true positives to the total number of positives \n",
    "            precision_ratio[m]=np.sum(np.logical_and(labels_ratio==1,Y_test_b==1))/np.sum(labels_ratio)\n",
    "            #Pandas Series.to_numpy() function is used to return a NumPy ndarray representing the values in given Series or Index.\n",
    "            ndcg_ratio[m]=ndcg_k(Y_test_b.to_numpy(),predicted_test,99)\n",
    "            \n",
    "            FN=np.sum(np.logical_and(predicted_test<cutoff_ratio, \\\n",
    "                                                          Y_test_b==1))\n",
    "            FP=np.sum(np.logical_and(predicted_test>=cutoff_ratio, \\\n",
    "                                                          Y_test_b==0))\n",
    "            # C_FN: Cost of a False Negative for ECM  -30\n",
    "            #C_FP: Cost of a False Positive for ECM  -1\n",
    "   \n",
    "            ecm_ratio[m]=C_FN*P_f*FN/n_P+C_FP*P_nf*FP/n_N\n",
    "            \n",
    "\n",
    "            clf_lr_raw = Logit(Y_train_b,add_constant(X_train_raw_b)).fit(disp=0)\n",
    "            predicted_test_raw=clf_lr_raw.predict(add_constant(X_test_raw_b))\n",
    "            predicted_test_raw=predicted_test_raw.to_numpy()\n",
    "                \n",
    "            cutoff_raw=np.percentile(predicted_test_raw,99) \n",
    "            labels_raw=(predicted_test_raw>=cutoff_raw).astype(int)\n",
    "            sensitivity_ratio_raw[m]=(np.sum(np.logical_and(labels_raw==1, \\\n",
    "                                                         Y_test_b==1))/np.sum(Y_test_b))\n",
    "            specificity_ratio_raw[m]=(np.sum(np.logical_and(labels_raw==0, \\\n",
    "                                                         Y_test_b==0))/np.sum(Y_test_b==0))\n",
    "            precision_ratio_raw[m]=(np.sum(np.logical_and(labels_raw==1, \\\n",
    "                                                         Y_test_b==1))/np.sum(labels_raw))\n",
    "            roc_set_ratio_raw[m]=(roc_auc_score(Y_test_b,predicted_test_raw))\n",
    "            ndcg_ratio_raw[m]=ndcg_k(Y_test_b.to_numpy(),predicted_test_raw,99)\n",
    "            \n",
    "            FN_raw=np.sum(np.logical_and(predicted_test_raw<cutoff_raw, \\\n",
    "                                                          Y_test_b==1))\n",
    "            FP_raw=np.sum(np.logical_and(predicted_test_raw>=cutoff_raw, \\\n",
    "                                                          Y_test_b==0))\n",
    "            # C_FN: Cost of a False Negative for ECM  -30\n",
    "            #C_FP: Cost of a False Positive for ECM  -1\n",
    "   \n",
    "            ecm_ratio_raw[m]=C_FN*P_f*FN_raw/n_P+C_FP*P_nf*FP_raw/n_N\n",
    "         \n",
    "            m+=1\n",
    "         \n",
    "        \n",
    "        t1=datetime.now()\n",
    "        dt=t1-t0\n",
    "        \n",
    "        f1_ratio=2*(precision_ratio*sensitivity_ratio)/(precision_ratio+sensitivity_ratio+1e-8)\n",
    "        f1_ratio_raw=2*(precision_ratio_raw*sensitivity_ratio_raw)/(precision_ratio_raw+sensitivity_ratio_raw+1e-8)\n",
    "        \n",
    "        # create performance table now\n",
    "        perf_tbl_general=pd.DataFrame()\n",
    "        perf_tbl_general['models']=['Logit']       \n",
    "        \n",
    "        perf_tbl_general['Roc']=[str(np.round(\n",
    "            np.mean(roc_set_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(roc_set_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Roc_noise_to_signal']=[str(np.round(\n",
    "            np.std(roc_set_ratio)/np.mean(roc_set_ratio)*100,2))+'%']\n",
    "                                                    \n",
    "        perf_tbl_general['Sensitivity @ 1 Prc']=[str(np.round(\n",
    "            np.mean(sensitivity_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(sensitivity_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Sensitivity_noise_to_signal']=[str(np.round(\n",
    "            np.std(sensitivity_ratio)/np.mean(sensitivity_ratio)*100,2))+'%']\n",
    "\n",
    "        perf_tbl_general['Specificity @ 1 Prc']=[str(np.round(\n",
    "            np.mean(specificity_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(specificity_ratio)*100,2))+'%)'] \n",
    "        \n",
    "        perf_tbl_general['Specificity_noise_to_signal']=[str(np.round(\n",
    "            np.std(specificity_ratio)/np.mean(specificity_ratio)*100,2))+'%']\n",
    "        \n",
    "\n",
    "        perf_tbl_general['Precision @ 1 Prc']=[str(np.round(\n",
    "            np.mean(precision_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(precision_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Precision_noise_to_signal']=[str(np.round(\n",
    "            np.std(precision_ratio)/np.mean(precision_ratio)*100,2))+'%']\n",
    "\n",
    "        perf_tbl_general['F1 Score @ 1 Prc']=[str(np.round(\n",
    "            np.mean(f1_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(f1_ratio)*100,2))+'%)']\n",
    "            \n",
    "        perf_tbl_general['F1 Score_noise_to_signal']=[str(np.round(\n",
    "            np.std(f1_ratio)/np.mean(f1_ratio)*100,2))+'%']\n",
    "        \n",
    "        perf_tbl_general['NDCG @ 1 Prc']=[str(np.round(\n",
    "            np.mean(ndcg_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(ndcg_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['NDCG_noise_to_signal']=[str(np.round(\n",
    "            np.std(ndcg_ratio)/np.mean(ndcg_ratio)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "        perf_tbl_general['ECM @ 1 Prc']=[str(np.round(\n",
    "            np.mean(ecm_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(ecm_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['ECM_noise_to_signal']=[str(np.round(\n",
    "            np.std(ecm_ratio)/np.mean(ecm_ratio)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "        \n",
    "        perf_tbl_general['Roc_raw']=[str(np.round(\n",
    "            np.mean(roc_set_ratio_raw)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(roc_set_ratio_raw)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Roc_noise_to_signal_raw']=[str(np.round(\n",
    "            np.std(roc_set_ratio_raw)/np.mean(roc_set_ratio_raw)*100,2))+'%']\n",
    "                                                    \n",
    "        perf_tbl_general['Sensitivity_raw @ 1 Prc']=[str(np.round(\n",
    "            np.mean(sensitivity_ratio_raw)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(sensitivity_ratio_raw)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Sensitivity_noise_to_signal_raw']=[str(np.round(\n",
    "            np.std(sensitivity_ratio_raw)/np.mean(sensitivity_ratio_raw)*100,2))+'%']\n",
    "\n",
    "        perf_tbl_general['Specificity_raw @ 1 Prc']=[str(np.round(\n",
    "            np.mean(specificity_ratio_raw)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(specificity_ratio_raw)*100,2))+'%)'] \n",
    "        \n",
    "        perf_tbl_general['Specificity_noise_to_signal_raw']=[str(np.round(\n",
    "            np.std(specificity_ratio_raw)/np.mean(specificity_ratio_raw)*100,2))+'%']\n",
    "        \n",
    "\n",
    "        perf_tbl_general['Precision_raw @ 1 Prc']=[str(np.round(\n",
    "            np.mean(precision_ratio_raw)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(precision_ratio_raw)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Precision_noise_to_signal_raw']=[str(np.round(\n",
    "            np.std(precision_ratio_raw)/np.mean(precision_ratio_raw)*100,2))+'%']\n",
    "\n",
    "        perf_tbl_general['F1 Score_raw @ 1 Prc']=[str(np.round(\n",
    "            np.mean(f1_ratio_raw)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(f1_ratio_raw)*100,2))+'%)']\n",
    "            \n",
    "        perf_tbl_general['F1 Score_noise_to_signal_raw']=[str(np.round(\n",
    "            np.std(f1_ratio_raw)/np.mean(f1_ratio_raw)*100,2))+'%']\n",
    "        \n",
    "        perf_tbl_general['NDCG_raw @ 1 Prc']=[str(np.round(\n",
    "            np.mean(ndcg_ratio_raw)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(ndcg_ratio_raw)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['NDCG_noise_to_signal_raw']=[str(np.round(\n",
    "            np.std(ndcg_ratio_raw)/np.mean(ndcg_ratio_raw)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "        perf_tbl_general['ECM_raw @ 1 Prc']=[str(np.round(\n",
    "            np.mean(ecm_ratio_raw)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(ecm_ratio_raw)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['ECM_noise_to_signal_raw']=[str(np.round(\n",
    "            np.std(ecm_ratio_raw)/np.mean(ecm_ratio_raw)*100,2))+'%']\n",
    "\n",
    "\n",
    "        \n",
    "        lbl_perf_tbl='MC_results_Logit'+',serial='+str(adjust_serial)+',B='+str(B)+'.csv'\n",
    "                        \n",
    "        if write==True:\n",
    "            perf_tbl_general.to_csv(lbl_perf_tbl,index=True)\n",
    "        \n",
    "        t001=datetime.now()\n",
    "        dt00=t001-t000\n",
    "        print('MC analysis is completed after '+str(dt00.total_seconds())+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0c8c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module initiated successfully ...\n",
      "Procedures are: mc_analysis()\n",
      "starting the MC analysis for case B=1000, serial treatment=True\n",
      "prior probablity of fraud between 1991-2000 is 0.78%\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "MC analysis is completed after 4.422353 seconds\n"
     ]
    }
   ],
   "source": [
    "a = ML_Fraud(sample_start = 1991,test_sample = range (2001,2011),OOS_per = 1,OOS_gap = 0,sampling = \"expanding\",adjust_serial = True,\n",
    "            cv_flag = False,cv_k = 10,write = True,IS_per = 10)\n",
    "a.mc_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751376af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
