{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9513b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile\n",
    "from extra_codes import calc_vif\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ML_Fraud:\n",
    "    __version__='1.0.5'\n",
    "    def __init__(self,sample_start=1991,test_sample=range(2001,2011),\n",
    "                 OOS_per=1,OOS_gap=0,sampling='expanding',adjust_serial=True,\n",
    "                 cv_type='kfold',temp_year=1,cv_flag=False,cv_k=10,write=True,IS_per=10):\n",
    "\n",
    "        if isfile('FraudDB2020.csv')==False:\n",
    "            df=pd.DataFrame()\n",
    "            for s in range(1,5):\n",
    "                fl_name='FraudDB2020_Part'+str(s)+'.csv'\n",
    "                new_df=pd.read_csv(fl_name)\n",
    "                df=df.append(new_df)\n",
    "            df.to_csv('FraudDB2020.csv',index=False)\n",
    "            \n",
    "        df=pd.read_csv('FraudDB2020.csv')\n",
    "        self.df=df\n",
    "        self.ss=sample_start\n",
    "        self.se=np.max(df.fyear)\n",
    "        self.ts=test_sample\n",
    "        self.cv_t=cv_type\n",
    "        self.cv=cv_flag\n",
    "        self.cv_k=cv_k\n",
    "        self.cv_t_y=temp_year\n",
    "        \n",
    "        sampling_set=['expanding','rolling']\n",
    "        if sampling in sampling_set:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Invalid sampling choice. Permitted options are \"expanding\" and \"rolling\"')\n",
    "        \n",
    "        self.sa=sampling\n",
    "        self.w=write\n",
    "        self.ip=IS_per\n",
    "        self.op=OOS_per\n",
    "        self.og=OOS_gap\n",
    "        self.a_s=adjust_serial\n",
    "        print('Module initiated successfully ...')\n",
    "        #The dir() function returns all properties and methods of the specified object, without the values.\n",
    "        list_methods=dir(self)\n",
    "        # .any: It checks for any element satisfying a condition and returns a True in case it finds any one element.\n",
    "        reduced_methods=[item+'()' for item in list_methods if any(['analy' in item,'compare' in item,item=='sumstats'])]\n",
    "        #string.join(iterable)\n",
    "        print('Procedures are: '+'; '.join(reduced_methods))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def analyse_ratio(self,C_FN=30,C_FP=1):\n",
    "        \"\"\"\n",
    "        This code uses 11 financial ratios to predict the likelihood of fraud in a financial statement.\n",
    "        \n",
    "        Parameters:\n",
    "            – C_FN: Cost of a False Negative for ECM\n",
    "            – C_FP: Cost of a False Positive for ECM\n",
    "\n",
    "        Predictive models:\n",
    "            – Support Vector Machine (SVM)\n",
    "            – Logistic Regression (LR)\n",
    "            – SGD Tree Boosting (SGD)\n",
    "            – Adaptive Boosting with Logistic Regression/LogitBoost (ADA)\n",
    "            – MUlti-layered Perceptron (MLP)\n",
    "            – FUSED (weighted average of estimated probs of other methods)\n",
    "\n",
    "        Outputs: \n",
    "        Main results are stored in the table variable \"perf_tbl_general\" written into\n",
    "        2 csv files: time period 2001-2010 and 2003-2008. \n",
    "\n",
    "        Steps:\n",
    "            1. Cross-validate to find optimal hyperparameters.\n",
    "            2. Estimating the performance for each OOS period.\n",
    "\n",
    "        Warnings: \n",
    "            – Running this code can take up to 85 mins. The cross-validation takes up\n",
    "            to 60 mins (you can skip this step) main analysis up to 15 mins. \n",
    "            These figures are estimates based on a MacBook Pro 2021.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        from imblearn.ensemble import RUSBoostClassifier\n",
    "        from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from extra_codes import ndcg_k,relogit\n",
    "        from statsmodels.discrete.discrete_model import Logit\n",
    "        from statsmodels.tools import add_constant\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        \n",
    "        t0=datetime.now()\n",
    "        # setting the parameters\n",
    "        IS_period=self.ip\n",
    "        k_fold=self.cv_k\n",
    "        OOS_period=self.op # 1 year ahead prediction\n",
    "        OOS_gap=self.og # Gap between training and testing period\n",
    "        start_OOS_year=self.ts[0] #2001\n",
    "        end_OOS_year=self.ts[-1] #2010\n",
    "        sample_start=self.ss #1991\n",
    "        adjust_serial=self.a_s\n",
    "        cv_type=self.cv_t\n",
    "        cross_val=self.cv\n",
    "        temp_year=self.cv_t_y #1\n",
    "        case_window=self.sa\n",
    "        fraud_df=self.df.copy(deep=True)\n",
    "        write=self.w\n",
    "\n",
    "        reduced_tbl_1=fraud_df.iloc[:,[0,1,3,7,8]]\n",
    "        reduced_tbl_2=fraud_df.iloc[:,-14:-3]\n",
    "        reduced_tblset=[reduced_tbl_1,reduced_tbl_2]\n",
    "        reduced_tbl=pd.concat(reduced_tblset,axis=1)\n",
    "        reduced_tbl=reduced_tbl[reduced_tbl.fyear>=sample_start] #1991\n",
    "        reduced_tbl=reduced_tbl[reduced_tbl.fyear<=end_OOS_year] #2010\n",
    "\n",
    "        # Setting the cross-validation setting\n",
    "        # IC sample: fyear 1991-2000\n",
    "        tbl_year_IS_CV=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear<start_OOS_year,\\\n",
    "                                                   reduced_tbl.fyear>=start_OOS_year-IS_period)]\n",
    "        tbl_year_IS_CV=tbl_year_IS_CV.reset_index(drop=True)\n",
    "        misstate_firms=np.unique(tbl_year_IS_CV.gvkey[tbl_year_IS_CV.AAER_DUMMY==1])\n",
    "\n",
    "        X_CV=tbl_year_IS_CV.iloc[:,-11:]\n",
    "\n",
    "        mean_vals=np.mean(X_CV)\n",
    "        std_vals=np.std(X_CV)\n",
    "        #Z-score: calculate the probability of a score occurring within a standard normal distribution\n",
    "        X_CV=(X_CV-mean_vals)/std_vals\n",
    "\n",
    "        Y_CV=tbl_year_IS_CV.AAER_DUMMY\n",
    "\n",
    "        P_f=np.sum(Y_CV==1)/len(Y_CV)\n",
    "        P_nf=1-P_f\n",
    "\n",
    "        print('prior probablity of fraud between '+str(sample_start)+'-'+\n",
    "              str(start_OOS_year-1)+' is '+str(np.round(P_f*100,2))+'%')\n",
    "        \n",
    "        range_oos=range(start_OOS_year,end_OOS_year+1,OOS_period) #(2001,2010+1,1)\n",
    "\n",
    "        roc_rus=np.zeros(len(range_oos))\n",
    "        sensitivity_OOS_rus1=np.zeros(len(range_oos))\n",
    "        specificity_OOS_rus1=np.zeros(len(range_oos))\n",
    "        precision_rus1=np.zeros(len(range_oos))\n",
    "        ndcg_rus1=np.zeros(len(range_oos))\n",
    "        ecm_rus1=np.zeros(len(range_oos))\n",
    "\n",
    "\n",
    "        roc_svm=np.zeros(len(range_oos))\n",
    "        sensitivity_OOS_svm1=np.zeros(len(range_oos))\n",
    "        specificity_OOS_svm1=np.zeros(len(range_oos))\n",
    "        precision_svm1=np.zeros(len(range_oos))\n",
    "        ndcg_svm1=np.zeros(len(range_oos))\n",
    "        ecm_svm1=np.zeros(len(range_oos))\n",
    "\n",
    "        roc_lr=np.zeros(len(range_oos))\n",
    "        sensitivity_OOS_lr1=np.zeros(len(range_oos))\n",
    "        specificity_OOS_lr1=np.zeros(len(range_oos))\n",
    "        precision_lr1=np.zeros(len(range_oos))\n",
    "        ndcg_lr1=np.zeros(len(range_oos))\n",
    "        ecm_lr1=np.zeros(len(range_oos))\n",
    "        \n",
    "\n",
    "        roc_sgd=np.zeros(len(range_oos))\n",
    "        sensitivity_OOS_sgd1=np.zeros(len(range_oos))\n",
    "        specificity_OOS_sgd1=np.zeros(len(range_oos))\n",
    "        precision_sgd1=np.zeros(len(range_oos))\n",
    "        ndcg_sgd1=np.zeros(len(range_oos))\n",
    "        ecm_sgd1=np.zeros(len(range_oos))\n",
    "\n",
    "        roc_ada=np.zeros(len(range_oos))\n",
    "        sensitivity_OOS_ada1=np.zeros(len(range_oos))\n",
    "        specificity_OOS_ada1=np.zeros(len(range_oos))\n",
    "        precision_ada1=np.zeros(len(range_oos))\n",
    "        ndcg_ada1=np.zeros(len(range_oos))\n",
    "        ecm_ada1=np.zeros(len(range_oos))\n",
    "\n",
    "\n",
    "        roc_mlp=np.zeros(len(range_oos))\n",
    "        sensitivity_OOS_mlp1=np.zeros(len(range_oos))\n",
    "        specificity_OOS_mlp1=np.zeros(len(range_oos))\n",
    "        precision_mlp1=np.zeros(len(range_oos))\n",
    "        ndcg_mlp1=np.zeros(len(range_oos))\n",
    "        ecm_mlp1=np.zeros(len(range_oos))\n",
    "\n",
    "\n",
    "        roc_fused=np.zeros(len(range_oos))\n",
    "        sensitivity_OOS_fused1=np.zeros(len(range_oos))\n",
    "        specificity_OOS_fused1=np.zeros(len(range_oos))\n",
    "        precision_fused1=np.zeros(len(range_oos))\n",
    "        ndcg_fused1=np.zeros(len(range_oos))\n",
    "        ecm_fused1=np.zeros(len(range_oos))\n",
    "        \n",
    "        \n",
    "        n_opt_rus=1000\n",
    "        r_opt_rus=1e-4\n",
    "        score_rus=0.6953935928499526\n",
    "                \n",
    "        opt_params_svm={'class_weight': {0: 0.01, 1: 1}, 'kernel': 'linear'}\n",
    "        C_opt=opt_params_svm['class_weight'][0]\n",
    "        kernel_opt=opt_params_svm['kernel']\n",
    "        score_svm=0.701939025416111\n",
    "                \n",
    "        score_lr=0.7056438104977343\n",
    "                \n",
    "        opt_params_sgd={'class_weight': {0: 5e-3, 1: 1}, 'loss': 'log', 'penalty': 'l2'}\n",
    "        score_sgd=0.7026775920776185\n",
    "    \n",
    "        opt_params_ada={'learning_rate': 0.9, 'n_estimators': 20}\n",
    "        score_ada=0.700229450411913\n",
    "                                \n",
    "        opt_params_mlp={'activation': 'logistic', 'hidden_layer_sizes': 5, 'solver': 'adam'}\n",
    "        score_mlp=0.706333862286029\n",
    "\n",
    "\n",
    "        m=0\n",
    "        for yr in range_oos: #2001-2010\n",
    "            t1=datetime.now()\n",
    "            if case_window=='expanding':\n",
    "                year_start_IS=sample_start #1991\n",
    "            else:\n",
    "                year_start_IS=yr-IS_period #1991\n",
    "            #how many years between training and testing sample: \n",
    "            #expanding: 1991-2000, 1991-2001\n",
    "            #rolling: 1991-2000, 1992-2001\n",
    "            tbl_year_IS=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear<yr-OOS_gap,\\\n",
    "                                                       reduced_tbl.fyear>=year_start_IS)]\n",
    "            tbl_year_IS=tbl_year_IS.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            misstate_firms=np.unique(tbl_year_IS.gvkey[tbl_year_IS.AAER_DUMMY==1])\n",
    "            #How many periods constitute the testing sample at a time: 2001, 2002\n",
    "            tbl_year_OOS=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear>=yr,\\\n",
    "                                                        reduced_tbl.fyear<yr+OOS_period)]\n",
    "            \n",
    "            if adjust_serial==True:\n",
    "                ok_index=np.zeros(tbl_year_OOS.shape[0])\n",
    "                for s in range(0,tbl_year_OOS.shape[0]):\n",
    "                    if not tbl_year_OOS.iloc[s,1] in misstate_firms:\n",
    "                        ok_index[s]=True\n",
    "                    \n",
    "                \n",
    "            else:\n",
    "                #filled with ones and keep all observations including serial frauds\n",
    "                ok_index=np.ones(tbl_year_OOS.shape[0]).astype(bool)\n",
    "                \n",
    "            #deleting observations where a company appears both in IS and OOS samples\n",
    "            tbl_year_OOS=tbl_year_OOS.iloc[ok_index==True,:]\n",
    "            tbl_year_OOS=tbl_year_OOS.reset_index(drop=True)\n",
    "                \n",
    "            \n",
    "            X=tbl_year_IS.iloc[:,-11:]\n",
    "            mean_vals=np.mean(X)\n",
    "            std_vals=np.std(X)\n",
    "            X=(X-mean_vals)/std_vals\n",
    "            Y=tbl_year_IS.AAER_DUMMY\n",
    "            \n",
    "            X_OOS=tbl_year_OOS.iloc[:,-11:]\n",
    "            X_OOS=(X_OOS-mean_vals)/std_vals\n",
    "            \n",
    "            Y_OOS=tbl_year_OOS.AAER_DUMMY\n",
    "            n_P=np.sum(Y_OOS==1)\n",
    "            n_N=np.sum(Y_OOS==0)\n",
    "            \n",
    "            scaling = MinMaxScaler(feature_range=(-1,1)).fit(X)\n",
    "            X = scaling.transform(X)\n",
    "            X_OOS = scaling.transform(X_OOS)\n",
    "            \n",
    "            \n",
    "            # Support Vector Machines\n",
    "            \n",
    "            clf_svm=SVC(class_weight={0:C_opt,1:1},kernel=kernel_opt,shrinking=False,\\\n",
    "                            probability=False,random_state=0,max_iter=-1,\\\n",
    "                                tol=X.shape[-1]*1e-3)\n",
    "                \n",
    "            clf_svm=clf_svm.fit(X,Y)\n",
    "            \n",
    "            pred_test_svm=clf_svm.decision_function(X_OOS)\n",
    "            probs_oos_fraud_svm=np.exp(pred_test_svm)/(1+np.exp(pred_test_svm))\n",
    "            \n",
    "            roc_svm[m]=roc_auc_score(Y_OOS,probs_oos_fraud_svm)\n",
    "            \n",
    "            \n",
    "            cutoff_OOS_svm=np.percentile(probs_oos_fraud_svm,99)\n",
    "            sensitivity_OOS_svm1[m]=np.sum(np.logical_and(probs_oos_fraud_svm>=cutoff_OOS_svm, \\\n",
    "                                                          Y_OOS==1))/np.sum(Y_OOS)\n",
    "            specificity_OOS_svm1[m]=np.sum(np.logical_and(probs_oos_fraud_svm<cutoff_OOS_svm, \\\n",
    "                                                          Y_OOS==0))/np.sum(Y_OOS==0)\n",
    "            precision_svm1[m]=np.sum(np.logical_and(probs_oos_fraud_svm>=cutoff_OOS_svm, \\\n",
    "                                                         Y_OOS==1))/np.sum(probs_oos_fraud_svm>=cutoff_OOS_svm)\n",
    "            ndcg_svm1[m]=ndcg_k(Y_OOS,probs_oos_fraud_svm,99)\n",
    "            \n",
    "            FN_svm1=np.sum(np.logical_and(probs_oos_fraud_svm<cutoff_OOS_svm, \\\n",
    "                                                          Y_OOS==1))\n",
    "            FP_svm1=np.sum(np.logical_and(probs_oos_fraud_svm>=cutoff_OOS_svm, \\\n",
    "                                                          Y_OOS==0))\n",
    "                \n",
    "            ecm_svm1[m]=C_FN*P_f*FN_svm1/n_P+C_FP*P_nf*FP_svm1/n_N\n",
    "                \n",
    "            \n",
    "            # Logistic Regression – Dechow et al (2011)\n",
    "            X_lr=add_constant(X)\n",
    "            X_OOS_lr=add_constant(X_OOS)\n",
    "            clf_lr = Logit(Y,X_lr)\n",
    "            clf_lr=clf_lr.fit(disp=0)\n",
    "            probs_oos_fraud_lr=clf_lr.predict(X_OOS_lr)\n",
    "\n",
    "            roc_lr[m]=roc_auc_score(Y_OOS,probs_oos_fraud_lr)\n",
    "            \n",
    "            \n",
    "            cutoff_OOS_lr=np.percentile(probs_oos_fraud_lr,99)\n",
    "            sensitivity_OOS_lr1[m]=np.sum(np.logical_and(probs_oos_fraud_lr>=cutoff_OOS_lr, \\\n",
    "                                                         Y_OOS==1))/np.sum(Y_OOS)\n",
    "            specificity_OOS_lr1[m]=np.sum(np.logical_and(probs_oos_fraud_lr<cutoff_OOS_lr, \\\n",
    "                                                          Y_OOS==0))/np.sum(Y_OOS==0)\n",
    "            precision_lr1[m]=np.sum(np.logical_and(probs_oos_fraud_lr>=cutoff_OOS_lr, \\\n",
    "                                                         Y_OOS==1))/np.sum(probs_oos_fraud_lr>=cutoff_OOS_lr)\n",
    "            ndcg_lr1[m]=ndcg_k(Y_OOS,probs_oos_fraud_lr,99)\n",
    "            \n",
    "            FN_lr1=np.sum(np.logical_and(probs_oos_fraud_lr<cutoff_OOS_lr, \\\n",
    "                                                          Y_OOS==1))\n",
    "            FP_lr1=np.sum(np.logical_and(probs_oos_fraud_lr>=cutoff_OOS_lr, \\\n",
    "                                                          Y_OOS==0))\n",
    "                \n",
    "            ecm_lr1[m]=C_FN*P_f*FN_lr1/n_P+C_FP*P_nf*FP_lr1/n_N\n",
    "                        \n",
    "            \n",
    "            # Stochastic Gradient Decent \n",
    "\n",
    "            clf_sgd=SGDClassifier(class_weight=opt_params_sgd['class_weight'],\\\n",
    "                                  loss=opt_params_sgd['loss'], random_state=0,\\\n",
    "                                   penalty=opt_params_sgd['penalty'],validation_fraction=.2,shuffle=False)\n",
    "            clf_sgd=clf_sgd.fit(X,Y)\n",
    "            probs_oos_fraud_sgd=clf_sgd.predict_proba(X_OOS)[:,-1]\n",
    "            \n",
    "            roc_sgd[m]=roc_auc_score(Y_OOS,probs_oos_fraud_sgd)\n",
    "            \n",
    "            cutoff_OOS_sgd=np.percentile(probs_oos_fraud_sgd,99)\n",
    "            sensitivity_OOS_sgd1[m]=np.sum(np.logical_and(probs_oos_fraud_sgd>=cutoff_OOS_sgd, \\\n",
    "                                                         Y_OOS==1))/np.sum(Y_OOS)\n",
    "            specificity_OOS_sgd1[m]=np.sum(np.logical_and(probs_oos_fraud_sgd<cutoff_OOS_sgd, \\\n",
    "                                                          Y_OOS==0))/np.sum(Y_OOS==0)\n",
    "            precision_sgd1[m]=np.sum(np.logical_and(probs_oos_fraud_sgd>=cutoff_OOS_sgd, \\\n",
    "                                                         Y_OOS==1))/np.sum(probs_oos_fraud_sgd>=cutoff_OOS_sgd)\n",
    "            ndcg_sgd1[m]=ndcg_k(Y_OOS,probs_oos_fraud_sgd,99)\n",
    "            \n",
    "            FN_sgd1=np.sum(np.logical_and(probs_oos_fraud_sgd<cutoff_OOS_sgd, \\\n",
    "                                                          Y_OOS==1))\n",
    "            FP_sgd1=np.sum(np.logical_and(probs_oos_fraud_sgd>=cutoff_OOS_sgd, \\\n",
    "                                                          Y_OOS==0))\n",
    "                \n",
    "            ecm_sgd1[m]=C_FN*P_f*FN_sgd1/n_P+C_FP*P_nf*FP_sgd1/n_N\n",
    "            \n",
    "            \n",
    "            # LogitBoost\n",
    "            base_lr=LogisticRegression(random_state=0,solver='newton-cg')\n",
    "            \n",
    "            \n",
    "            clf_ada=AdaBoostClassifier(n_estimators=opt_params_ada['n_estimators'],\\\n",
    "                                       learning_rate=opt_params_ada['learning_rate'],\\\n",
    "                                           base_estimator=base_lr,random_state=0)\n",
    "            clf_ada=clf_ada.fit(X,Y)\n",
    "            probs_oos_fraud_ada=clf_ada.predict_proba(X_OOS)[:,-1]\n",
    "            \n",
    "            \n",
    "            labels_ada=clf_ada.predict(X_OOS)\n",
    "            \n",
    "            roc_ada[m]=roc_auc_score(Y_OOS,probs_oos_fraud_ada)\n",
    "            cutoff_OOS_ada=np.percentile(probs_oos_fraud_ada,99)\n",
    "            sensitivity_OOS_ada1[m]=np.sum(np.logical_and(probs_oos_fraud_ada>=cutoff_OOS_ada, \\\n",
    "                                                         Y_OOS==1))/np.sum(Y_OOS)\n",
    "            specificity_OOS_ada1[m]=np.sum(np.logical_and(probs_oos_fraud_ada<cutoff_OOS_ada, \\\n",
    "                                                          Y_OOS==0))/np.sum(Y_OOS==0)\n",
    "            precision_ada1[m]=np.sum(np.logical_and(probs_oos_fraud_ada>=cutoff_OOS_ada, \\\n",
    "                                                         Y_OOS==1))/np.sum(probs_oos_fraud_ada>=cutoff_OOS_ada)\n",
    "            ndcg_ada1[m]=ndcg_k(Y_OOS,probs_oos_fraud_ada,99)\n",
    "            \n",
    "            FN_ada1=np.sum(np.logical_and(probs_oos_fraud_ada<cutoff_OOS_ada, \\\n",
    "                                                          Y_OOS==1))\n",
    "            FP_ada1=np.sum(np.logical_and(probs_oos_fraud_ada>=cutoff_OOS_ada, \\\n",
    "                                                          Y_OOS==0))\n",
    "                \n",
    "            ecm_ada1[m]=C_FN*P_f*FN_ada1/n_P+C_FP*P_nf*FP_ada1/n_N\n",
    "                \n",
    "            \n",
    "            # Multi Layer Perceptron\n",
    "            clf_mlp=MLPClassifier(hidden_layer_sizes=opt_params_mlp['hidden_layer_sizes'], \\\n",
    "                                  activation=opt_params_mlp['activation'],solver=opt_params_mlp['solver'],\\\n",
    "                                               random_state=0,validation_fraction=.1)\n",
    "            clf_mlp=clf_mlp.fit(X,Y)\n",
    "            probs_oos_fraud_mlp=clf_mlp.predict_proba(X_OOS)[:,-1]\n",
    "                        \n",
    "            roc_mlp[m]=roc_auc_score(Y_OOS,probs_oos_fraud_mlp)\n",
    "            \n",
    "            cutoff_OOS_mlp=np.percentile(probs_oos_fraud_mlp,99)\n",
    "            sensitivity_OOS_mlp1[m]=np.sum(np.logical_and(probs_oos_fraud_mlp>=cutoff_OOS_mlp, \\\n",
    "                                                         Y_OOS==1))/np.sum(Y_OOS)\n",
    "            specificity_OOS_mlp1[m]=np.sum(np.logical_and(probs_oos_fraud_mlp<cutoff_OOS_mlp, \\\n",
    "                                                          Y_OOS==0))/np.sum(Y_OOS==0)\n",
    "            precision_mlp1[m]=np.sum(np.logical_and(probs_oos_fraud_mlp>=cutoff_OOS_mlp, \\\n",
    "                                                         Y_OOS==1))/np.sum(probs_oos_fraud_mlp>=cutoff_OOS_mlp)\n",
    "            ndcg_mlp1[m]=ndcg_k(Y_OOS,probs_oos_fraud_mlp,99)\n",
    "            \n",
    "            FN_mlp1=np.sum(np.logical_and(probs_oos_fraud_mlp<cutoff_OOS_mlp, \\\n",
    "                                                          Y_OOS==1))\n",
    "            FP_mlp1=np.sum(np.logical_and(probs_oos_fraud_mlp>=cutoff_OOS_mlp, \\\n",
    "                                                          Y_OOS==0))\n",
    "                \n",
    "            ecm_mlp1[m]=C_FN*P_f*FN_mlp1/n_P+C_FP*P_nf*FP_mlp1/n_N\n",
    "                \n",
    "            \n",
    "            \n",
    "            # Fused approach\n",
    "            #highest fine tunning scores for each model\n",
    "            weight_ser=np.array([score_svm,score_lr,score_sgd,score_ada,score_mlp])\n",
    "            #give each model weight based on their score ranking\n",
    "            weight_ser=weight_ser/np.sum(weight_ser)\n",
    "            # ** Exponentiation\n",
    "            probs_oos_fraud_svm=(1+np.exp(-1*probs_oos_fraud_svm))**-1\n",
    "                \n",
    "            probs_oos_fraud_lr=(1+np.exp(-1*probs_oos_fraud_lr))**-1\n",
    "                \n",
    "            probs_oos_fraud_sgd=(1+np.exp(-1*probs_oos_fraud_sgd))**-1\n",
    "            \n",
    "            probs_oos_fraud_ada=(1+np.exp(-1*probs_oos_fraud_ada))**-1\n",
    "                \n",
    "            probs_oos_fraud_mlp=(1+np.exp(-1*probs_oos_fraud_mlp))**-1\n",
    "            \n",
    "            #dot.product   - weighted probability of the classification made by each model\n",
    "            clf_fused=np.dot(np.array([probs_oos_fraud_svm,\\\n",
    "                                  probs_oos_fraud_lr,probs_oos_fraud_sgd,probs_oos_fraud_ada,\\\n",
    "                                      probs_oos_fraud_mlp]).T,weight_ser)\n",
    "            \n",
    "            probs_oos_fraud_fused=clf_fused\n",
    "                        \n",
    "            roc_fused[m]=roc_auc_score(Y_OOS,probs_oos_fraud_fused)\n",
    "            \n",
    "            \n",
    "            cutoff_OOS_fused=np.percentile(probs_oos_fraud_fused,99)\n",
    "            sensitivity_OOS_fused1[m]=np.sum(np.logical_and(probs_oos_fraud_fused>=cutoff_OOS_fused, \\\n",
    "                                                         Y_OOS==1))/np.sum(Y_OOS)\n",
    "            specificity_OOS_fused1[m]=np.sum(np.logical_and(probs_oos_fraud_fused<cutoff_OOS_fused, \\\n",
    "                                                          Y_OOS==0))/np.sum(Y_OOS==0)\n",
    "            precision_fused1[m]=np.sum(np.logical_and(probs_oos_fraud_fused>=cutoff_OOS_fused, \\\n",
    "                                                         Y_OOS==1))/np.sum(probs_oos_fraud_fused>=cutoff_OOS_fused)\n",
    "            ndcg_fused1[m]=ndcg_k(Y_OOS,probs_oos_fraud_fused,99)\n",
    "            \n",
    "            FN_fused1=np.sum(np.logical_and(probs_oos_fraud_fused<cutoff_OOS_fused, \\\n",
    "                                                          Y_OOS==1))\n",
    "            FP_fused1=np.sum(np.logical_and(probs_oos_fraud_fused>=cutoff_OOS_fused, \\\n",
    "                                                          Y_OOS==0))\n",
    "                \n",
    "            ecm_fused1[m]=C_FN*P_f*FN_fused1/n_P+C_FP*P_nf*FP_fused1/n_N\n",
    "            \n",
    "\n",
    "            \n",
    "            t2=datetime.now() \n",
    "            dt=t2-t1\n",
    "            print('analysis finished for OOS period '+str(yr)+' after '+str(dt.total_seconds())+' sec')\n",
    "            m+=1\n",
    "\n",
    "        print('average top percentile sensitivity for the period '+str(start_OOS_year)+' to '+\\\n",
    "              str(end_OOS_year)+' is '+ str(round(np.mean(sensitivity_OOS_svm1)*100,2))+\\\n",
    "                          '% for SVM vs '+ str(round(np.mean(sensitivity_OOS_lr1)*100,2))+\\\n",
    "                          '% for Dechow-LR vs '+ str(round(np.mean(sensitivity_OOS_sgd1)*100,2))+\\\n",
    "                              '% for SGD vs '+ str(round(np.mean(sensitivity_OOS_ada1)*100,2))+\\\n",
    "                                  '% for ADA vs '+ str(round(np.mean(sensitivity_OOS_mlp1)*100,2))+\\\n",
    "                                      '% for MLP vs '+ str(round(np.mean(sensitivity_OOS_fused1)*100,2))+\\\n",
    "                                          '% for FUSED')\n",
    "\n",
    "        \n",
    "        f1_score_svm1=2*(precision_svm1*sensitivity_OOS_svm1)/\\\n",
    "            (precision_svm1+sensitivity_OOS_svm1+1e-8)\n",
    "        \n",
    "        f1_score_lr1=2*(precision_lr1*sensitivity_OOS_lr1)/\\\n",
    "            (precision_lr1+sensitivity_OOS_lr1+1e-8)\n",
    "        \n",
    "        f1_score_sgd1=2*(precision_sgd1*sensitivity_OOS_sgd1)/\\\n",
    "            (precision_sgd1+sensitivity_OOS_sgd1+1e-8)\n",
    "        \n",
    "        f1_score_ada1=2*(precision_ada1*sensitivity_OOS_ada1)/\\\n",
    "            (precision_ada1+sensitivity_OOS_ada1+1e-8)\n",
    "        \n",
    "        f1_score_mlp1=2*(precision_mlp1*sensitivity_OOS_mlp1)/\\\n",
    "            (precision_mlp1+sensitivity_OOS_mlp1+1e-8)\n",
    "        \n",
    "        f1_score_fused1=2*(precision_fused1*sensitivity_OOS_fused1)/\\\n",
    "            (precision_fused1+sensitivity_OOS_fused1+1e-8)\n",
    "        \n",
    "        # create performance table now\n",
    "        perf_tbl_general=pd.DataFrame()\n",
    "        perf_tbl_general['models']=['SVM','LR','SGD','LogitBoost','MLP','FUSED']\n",
    "        perf_tbl_general['Roc']=[str(np.round(\n",
    "                np.mean(roc_svm)*100,2))+'% ('+\\\n",
    "                str(np.round(np.std(roc_svm)*100,2))+'%)',str(np.round(\n",
    "                    np.mean(roc_lr)*100,2))+'% ('+\\\n",
    "                    str(np.round(np.std(roc_lr)*100,2))+'%)',str(np.round(\n",
    "                        np.mean(roc_sgd)*100,2))+'% ('+\\\n",
    "                        str(np.round(np.std(roc_sgd)*100,2))+'%)',str(np.round(\n",
    "                            np.mean(roc_ada)*100,2))+'% ('+\\\n",
    "                            str(np.round(np.std(roc_ada)*100,2))+'%)',str(np.round(\n",
    "                                np.mean(roc_mlp)*100,2))+'% ('+\\\n",
    "                                str(np.round(np.std(roc_mlp)*100,2))+'%)',\n",
    "                                str(np.round(\n",
    "                                    np.mean(roc_fused)*100,2))+'% ('+\\\n",
    "                                    str(np.round(np.std(roc_fused)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Roc_noise_to_signal']=[str(np.round(\n",
    "                np.std(roc_svm)/np.mean(roc_svm)*100,2))+'%',str(np.round(\n",
    "                    np.std(roc_lr)/np.mean(roc_lr)*100,2))+'%',str(np.round(\n",
    "                        np.std(roc_sgd)/np.mean(roc_sgd)*100,2))+'%',str(np.round(\n",
    "                            np.std(roc_ada)/np.mean(roc_ada)*100,2))+'%',str(np.round(\n",
    "                                np.std(roc_mlp)/np.mean(roc_mlp)*100,2))+'%',\n",
    "                                str(np.round(np.std(roc_fused)/np.mean(roc_fused)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "\n",
    "                                                    \n",
    "        perf_tbl_general['Sensitivity @ 1 Prc']=[str(np.round(\n",
    "                np.mean(sensitivity_OOS_svm1)*100,2))+'% ('+\\\n",
    "                str(np.round(np.std(sensitivity_OOS_svm1)*100,2))+'%)',str(np.round(\n",
    "                    np.mean(sensitivity_OOS_lr1)*100,2))+'% ('+\\\n",
    "                    str(np.round(np.std(sensitivity_OOS_lr1)*100,2))+'%)',str(np.round(\n",
    "                        np.mean(sensitivity_OOS_sgd1)*100,2))+'% ('+\\\n",
    "                        str(np.round(np.std(sensitivity_OOS_sgd1)*100,2))+'%)',str(np.round(\n",
    "                            np.mean(sensitivity_OOS_ada1)*100,2))+'% ('+\\\n",
    "                            str(np.round(np.std(sensitivity_OOS_ada1)*100,2))+'%)',str(np.round(\n",
    "                                np.mean(sensitivity_OOS_mlp1)*100,2))+'% ('+\\\n",
    "                                str(np.round(np.std(sensitivity_OOS_mlp1)*100,2))+'%)',\n",
    "                                str(np.round(\n",
    "                                    np.mean(sensitivity_OOS_fused1)*100,2))+'% ('+\\\n",
    "                                    str(np.round(np.std(sensitivity_OOS_fused1)*100,2))+'%)']\n",
    "        \n",
    "        \n",
    "        perf_tbl_general['Sensitivity_noise_to_signal @ 1 Prc']=[str(np.round(\n",
    "                np.std(sensitivity_OOS_svm1)/np.mean(sensitivity_OOS_svm1)*100,2))+'%',str(np.round(\n",
    "                    np.std(sensitivity_OOS_lr1)/np.mean(sensitivity_OOS_lr1)*100,2))+'%',str(np.round(\n",
    "                        np.std(sensitivity_OOS_sgd1)/np.mean(sensitivity_OOS_sgd1)*100,2))+'%',str(np.round(\n",
    "                            np.std(sensitivity_OOS_ada1)/np.mean(sensitivity_OOS_ada1)*100,2))+'%',str(np.round(\n",
    "                                np.std(sensitivity_OOS_mlp1)/np.mean(sensitivity_OOS_mlp1)*100,2))+'%',\n",
    "                                str(np.round(np.std(sensitivity_OOS_fused1)/np.mean(sensitivity_OOS_fused1)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "\n",
    "        perf_tbl_general['Specificity @ 1 Prc']=[str(np.round(\n",
    "                np.mean(specificity_OOS_svm1)*100,2))+'% ('+\\\n",
    "                str(np.round(np.std(specificity_OOS_svm1)*100,2))+'%)',str(np.round(\n",
    "                    np.mean(specificity_OOS_lr1)*100,2))+'% ('+\\\n",
    "                    str(np.round(np.std(specificity_OOS_lr1)*100,2))+'%)',str(np.round(\n",
    "                        np.mean(specificity_OOS_sgd1)*100,2))+'% ('+\\\n",
    "                        str(np.round(np.std(specificity_OOS_sgd1)*100,2))+'%)',str(np.round(\n",
    "                            np.mean(specificity_OOS_ada1)*100,2))+'% ('+\\\n",
    "                            str(np.round(np.std(specificity_OOS_ada1)*100,2))+'%)',str(np.round(\n",
    "                                np.mean(specificity_OOS_mlp1)*100,2))+'% ('+\\\n",
    "                                str(np.round(np.std(specificity_OOS_mlp1)*100,2))+'%)',\n",
    "                                str(np.round(np.mean(specificity_OOS_fused1)*100,2))+'% ('+\\\n",
    "                                    str(np.round(np.std(specificity_OOS_fused1)*100,2))+'%)']\n",
    "        \n",
    "        \n",
    "        perf_tbl_general['Specificity_noise_to_signal @ 1 Prc']=[str(np.round(\n",
    "                np.std(specificity_OOS_svm1)/np.mean(specificity_OOS_svm1)*100,2))+'%',str(np.round(\n",
    "                    np.std(specificity_OOS_lr1)/np.mean(specificity_OOS_lr1)*100,2))+'%',str(np.round(\n",
    "                        np.std(specificity_OOS_sgd1)/np.mean(specificity_OOS_sgd1)*100,2))+'%',str(np.round(\n",
    "                            np.std(specificity_OOS_ada1)/np.mean(specificity_OOS_ada1)*100,2))+'%',str(np.round(\n",
    "                                np.std(specificity_OOS_mlp1)/np.mean(specificity_OOS_mlp1)*100,2))+'%',\n",
    "                                str(np.round(np.std(specificity_OOS_fused1)/np.mean(specificity_OOS_fused1)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        perf_tbl_general['Precision @ 1 Prc']=[str(np.round(\n",
    "                np.mean(precision_svm1)*100,2))+'% ('+\\\n",
    "                str(np.round(np.std(precision_svm1)*100,2))+'%)',str(np.round(\n",
    "                    np.mean(precision_lr1)*100,2))+'% ('+\\\n",
    "                    str(np.round(np.std(precision_lr1)*100,2))+'%)',str(np.round(\n",
    "                        np.mean(precision_sgd1)*100,2))+'% ('+\\\n",
    "                        str(np.round(np.std(precision_sgd1)*100,2))+'%)',str(np.round(\n",
    "                            np.mean(precision_ada1)*100,2))+'% ('+\\\n",
    "                            str(np.round(np.std(precision_ada1)*100,2))+'%)',str(np.round(\n",
    "                                np.mean(precision_mlp1)*100,2))+'% ('+\\\n",
    "                                str(np.round(np.std(precision_mlp1)*100,2))+'%)',\n",
    "                                str(np.round(\n",
    "                                    np.mean(precision_fused1)*100,2))+'% ('+\\\n",
    "                                    str(np.round(np.std(precision_fused1)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Precision_noise_to_signal @ 1 Prc']=[str(np.round(\n",
    "                np.std(precision_svm1)/np.mean(precision_svm1)*100,2))+'%',str(np.round(\n",
    "                    np.std(precision_lr1)/np.mean(precision_lr1)*100,2))+'%',str(np.round(\n",
    "                        np.std(precision_sgd1)/np.mean(precision_sgd1)*100,2))+'%',str(np.round(\n",
    "                            np.std(precision_ada1)/np.mean(precision_ada1)*100,2))+'%',str(np.round(\n",
    "                                np.std(precision_mlp1)/np.mean(precision_mlp1)*100,2))+'%',\n",
    "                                str(np.round(np.std(precision_fused1)/np.mean(precision_fused1)*100,2))+'%']\n",
    "                                    \n",
    "                                    \n",
    "        perf_tbl_general['F1 Score @ 1 Prc']=[str(np.round(\n",
    "                np.mean(f1_score_svm1)*100,2))+'% ('+\\\n",
    "                str(np.round(np.std(f1_score_svm1)*100,2))+'%)',str(np.round(\n",
    "                    np.mean(f1_score_lr1)*100,2))+'% ('+\\\n",
    "                    str(np.round(np.std(f1_score_lr1)*100,2))+'%)',str(np.round(\n",
    "                        np.mean(f1_score_sgd1)*100,2))+'% ('+\\\n",
    "                        str(np.round(np.std(f1_score_sgd1)*100,2))+'%)',str(np.round(\n",
    "                            np.mean(f1_score_ada1)*100,2))+'% ('+\\\n",
    "                            str(np.round(np.std(f1_score_ada1)*100,2))+'%)',str(np.round(\n",
    "                                np.mean(f1_score_mlp1)*100,2))+'% ('+\\\n",
    "                                str(np.round(np.std(f1_score_mlp1)*100,2))+'%)',\n",
    "                                str(np.round(\n",
    "                                    np.mean(f1_score_fused1)*100,2))+'% ('+\\\n",
    "                                    str(np.round(np.std(f1_score_fused1)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['F1 Score_noise_to_signal @ 1 Prc']=[str(np.round(\n",
    "                np.std(f1_score_svm1)/np.mean(f1_score_svm1)*100,2))+'%',str(np.round(\n",
    "                    np.std(f1_score_lr1)/np.mean(f1_score_lr1)*100,2))+'%',str(np.round(\n",
    "                        np.std(f1_score_sgd1)/np.mean(f1_score_sgd1)*100,2))+'%',str(np.round(\n",
    "                            np.std(f1_score_ada1)/np.mean(f1_score_ada1)*100,2))+'%',str(np.round(\n",
    "                                np.std(f1_score_mlp1)/np.mean(f1_score_mlp1)*100,2))+'%',\n",
    "                                str(np.round(np.std(f1_score_fused1)/np.mean(f1_score_fused1)*100,2))+'%']\n",
    "            \n",
    "        \n",
    "        perf_tbl_general['NDCG @ 1 Prc']=[str(np.round(\n",
    "                np.mean(ndcg_svm1)*100,2))+'% ('+\\\n",
    "                str(np.round(np.std(ndcg_svm1)*100,2))+'%)',str(np.round(\n",
    "                    np.mean(ndcg_lr1)*100,2))+'% ('+\\\n",
    "                    str(np.round(np.std(ndcg_lr1)*100,2))+'%)',str(np.round(\n",
    "                        np.mean(ndcg_sgd1)*100,2))+'% ('+\\\n",
    "                        str(np.round(np.std(ndcg_sgd1)*100,2))+'%)',str(np.round(\n",
    "                            np.mean(ndcg_ada1)*100,2))+'% ('+\\\n",
    "                            str(np.round(np.std(ndcg_ada1)*100,2))+'%)',str(np.round(\n",
    "                                np.mean(ndcg_mlp1)*100,2))+'% ('+\\\n",
    "                                str(np.round(np.std(ndcg_mlp1)*100,2))+'%)',\n",
    "                                str(np.round(\n",
    "                                    np.mean(ndcg_fused1)*100,2))+'% ('+\\\n",
    "                                    str(np.round(np.std(ndcg_fused1)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['NDCG_noise_to_signal @ 1 Prc']=[str(np.round(\n",
    "                np.std(ndcg_svm1)/np.mean(ndcg_svm1)*100,2))+'%',str(np.round(\n",
    "                    np.std(ndcg_lr1)/np.mean(ndcg_lr1)*100,2))+'%',str(np.round(\n",
    "                        np.std(ndcg_sgd1)/np.mean(ndcg_sgd1)*100,2))+'%',str(np.round(\n",
    "                            np.std(ndcg_ada1)/np.mean(ndcg_ada1)*100,2))+'%',str(np.round(\n",
    "                                np.std(ndcg_mlp1)/np.mean(ndcg_mlp1)*100,2))+'%',\n",
    "                                str(np.round(np.std(ndcg_fused1)/np.mean(ndcg_fused1)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        perf_tbl_general['ECM @ 1 Prc']=[str(np.round(\n",
    "                np.mean(ecm_svm1)*100,2))+'% ('+\\\n",
    "                str(np.round(np.std(ecm_svm1)*100,2))+'%)',str(np.round(\n",
    "                    np.mean(ecm_lr1)*100,2))+'% ('+\\\n",
    "                    str(np.round(np.std(ecm_lr1)*100,2))+'%)',str(np.round(\n",
    "                        np.mean(ecm_sgd1)*100,2))+'% ('+\\\n",
    "                        str(np.round(np.std(ecm_sgd1)*100,2))+'%)',str(np.round(\n",
    "                            np.mean(ecm_ada1)*100,2))+'% ('+\\\n",
    "                            str(np.round(np.std(ecm_ada1)*100,2))+'%)',str(np.round(\n",
    "                                np.mean(ecm_mlp1)*100,2))+'% ('+\\\n",
    "                                str(np.round(np.std(ecm_mlp1)*100,2))+'%)',\n",
    "                                str(np.round(\n",
    "                                    np.mean(ecm_fused1)*100,2))+'% ('+\\\n",
    "                                    str(np.round(np.std(ecm_fused1)*100,2))+'%)']\n",
    "        \n",
    "        \n",
    "        perf_tbl_general['ECM_noise_to_signal @ 1 Prc']=[str(np.round(\n",
    "                np.std(ecm_svm1)/np.mean(ecm_svm1)*100,2))+'%',str(np.round(\n",
    "                    np.std(ecm_lr1)/np.mean(ecm_lr1)*100,2))+'%',str(np.round(\n",
    "                        np.std(ecm_sgd1)/np.mean(ecm_sgd1)*100,2))+'%',str(np.round(\n",
    "                            np.std(ecm_ada1)/np.mean(ecm_ada1)*100,2))+'%',str(np.round(\n",
    "                                np.std(ecm_mlp1)/np.mean(ecm_mlp1)*100,2))+'%',\n",
    "                                str(np.round(np.std(ecm_fused1)/np.mean(ecm_fused1)*100,2))+'%']\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "        lbl_perf_tbl='perf_tbl_'+str(start_OOS_year)+'_'+str(end_OOS_year)+\\\n",
    "                    '_'+case_window+',OOS='+str(OOS_period)+','+\\\n",
    "                    str(k_fold)+'fold'+',serial='+str(adjust_serial)+\\\n",
    "                    ',gap='+str(OOS_gap)+'_6ratios_kfold.csv'\n",
    "\n",
    "\n",
    "\n",
    "        if write==True:\n",
    "            perf_tbl_general.to_csv(lbl_perf_tbl,index=False)\n",
    "        print(perf_tbl_general)\n",
    "        t_last=datetime.now()\n",
    "        dt_total=t_last-t0\n",
    "        print('total run time is '+str(dt_total.total_seconds())+' sec')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23b3cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module initiated successfully ...\n",
      "Procedures are: analyse_ratio()\n",
      "prior probablity of fraud between 1991-2000 is 0.78%\n",
      "analysis finished for OOS period 2001 after 24.121504 sec\n",
      "analysis finished for OOS period 2002 after 30.244797 sec\n",
      "analysis finished for OOS period 2003 after 36.652687 sec\n",
      "analysis finished for OOS period 2004 after 43.547498 sec\n",
      "analysis finished for OOS period 2005 after 50.652381 sec\n",
      "analysis finished for OOS period 2006 after 68.725007 sec\n",
      "analysis finished for OOS period 2007 after 84.684954 sec\n",
      "analysis finished for OOS period 2008 after 91.410226 sec\n",
      "analysis finished for OOS period 2009 after 105.368471 sec\n",
      "analysis finished for OOS period 2010 after 117.555866 sec\n",
      "average top percentile sensitivity for the period 2001 to 2010 is 7.33% for SVM vs 7.33% for Dechow-LR vs 8.04% for SGD vs 6.38% for ADA vs 5.54% for MLP vs 8.04% for FUSED\n",
      "       models              Roc Roc_noise_to_signal Sensitivity @ 1 Prc  \\\n",
      "0         SVM  63.17% (11.49%)               18.2%      7.33% (11.47%)   \n",
      "1          LR   63.79% (11.9%)              18.65%      7.33% (11.47%)   \n",
      "2         SGD  62.54% (11.98%)              19.15%      8.04% (11.21%)   \n",
      "3  LogitBoost  61.47% (12.03%)              19.58%       6.38% (9.71%)   \n",
      "4         MLP  64.41% (11.33%)              17.59%       5.54% (9.86%)   \n",
      "5       FUSED  62.94% (11.81%)              18.77%      8.04% (11.21%)   \n",
      "\n",
      "  Sensitivity_noise_to_signal @ 1 Prc Specificity @ 1 Prc  \\\n",
      "0                             156.49%       99.0% (0.03%)   \n",
      "1                             156.49%       99.0% (0.03%)   \n",
      "2                             139.37%      99.01% (0.03%)   \n",
      "3                             152.17%       99.0% (0.02%)   \n",
      "4                             177.79%       99.0% (0.02%)   \n",
      "5                             139.37%      99.01% (0.03%)   \n",
      "\n",
      "  Specificity_noise_to_signal @ 1 Prc Precision @ 1 Prc  \\\n",
      "0                               0.03%      1.8% (2.77%)   \n",
      "1                               0.03%      1.8% (2.77%)   \n",
      "2                               0.03%     2.08% (2.72%)   \n",
      "3                               0.02%     1.47% (1.48%)   \n",
      "4                               0.02%     1.17% (1.44%)   \n",
      "5                               0.03%     2.08% (2.72%)   \n",
      "\n",
      "  Precision_noise_to_signal @ 1 Prc F1 Score @ 1 Prc  \\\n",
      "0                           154.18%    2.74% (4.14%)   \n",
      "1                           154.18%    2.74% (4.14%)   \n",
      "2                           130.88%    3.14% (4.05%)   \n",
      "3                           100.58%    2.26% (2.33%)   \n",
      "4                            123.2%    1.81% (2.29%)   \n",
      "5                           130.88%    3.14% (4.05%)   \n",
      "\n",
      "  F1 Score_noise_to_signal @ 1 Prc   NDCG @ 1 Prc  \\\n",
      "0                          151.02%   3.5% (5.33%)   \n",
      "1                          151.02%  3.25% (4.82%)   \n",
      "2                          128.88%  5.26% (6.14%)   \n",
      "3                          103.24%  3.41% (3.66%)   \n",
      "4                          126.56%  2.16% (3.12%)   \n",
      "5                          128.88%  4.13% (5.35%)   \n",
      "\n",
      "  NDCG_noise_to_signal @ 1 Prc     ECM @ 1 Prc ECM_noise_to_signal @ 1 Prc  \n",
      "0                      152.61%   22.61% (2.7%)                      11.94%  \n",
      "1                      148.27%   22.61% (2.7%)                      11.94%  \n",
      "2                      116.71%  22.44% (2.64%)                      11.76%  \n",
      "3                      107.27%  22.83% (2.28%)                       9.97%  \n",
      "4                      144.31%  23.03% (2.31%)                      10.04%  \n",
      "5                      129.74%  22.44% (2.64%)                      11.76%  \n",
      "total run time is 653.026273 sec\n"
     ]
    }
   ],
   "source": [
    "a = ML_Fraud(sample_start = 1991,test_sample = range (2001,2011),OOS_per = 1,OOS_gap = 0,sampling = \"expanding\",adjust_serial = True,\n",
    "            cv_flag = False,cv_k = 10,write = True,IS_per = 10)\n",
    "a.analyse_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcbf48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
