{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f17e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile\n",
    "from extra_codes import calc_vif\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ML_Fraud:\n",
    "    __version__='1.0.5'\n",
    "    def __init__(self,sample_start=1991,test_sample=range(2001,2011),\n",
    "                 OOS_per=1,OOS_gap=0,sampling='expanding',adjust_serial=True,\n",
    "                 cv_type='kfold',temp_year=1,cv_flag=False,cv_k=10,write=True,IS_per=10):\n",
    "\n",
    "        if isfile('FraudDB2020.csv')==False:\n",
    "            df=pd.DataFrame()\n",
    "            for s in range(1,5):\n",
    "                fl_name='FraudDB2020_Part'+str(s)+'.csv'\n",
    "                new_df=pd.read_csv(fl_name)\n",
    "                df=df.append(new_df)\n",
    "            df.to_csv('FraudDB2020.csv',index=False)\n",
    "            \n",
    "        df=pd.read_csv('FraudDB2020.csv')\n",
    "        self.df=df\n",
    "        self.ss=sample_start\n",
    "        self.se=np.max(df.fyear)\n",
    "        self.ts=test_sample\n",
    "        self.cv_t=cv_type\n",
    "        self.cv=cv_flag\n",
    "        self.cv_k=cv_k\n",
    "        self.cv_t_y=temp_year\n",
    "        \n",
    "        sampling_set=['expanding','rolling']\n",
    "        if sampling in sampling_set:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Invalid sampling choice. Permitted options are \"expanding\" and \"rolling\"')\n",
    "        \n",
    "        self.sa=sampling\n",
    "        self.w=write\n",
    "        self.ip=IS_per\n",
    "        self.op=OOS_per\n",
    "        self.og=OOS_gap\n",
    "        self.a_s=adjust_serial\n",
    "        print('Module initiated successfully ...')\n",
    "        #The dir() function returns all properties and methods of the specified object, without the values.\n",
    "        list_methods=dir(self)\n",
    "        # .any: It checks for any element satisfying a condition and returns a True in case it finds any one element.\n",
    "        reduced_methods=[item+'()' for item in list_methods if any(['analy' in item,'compare' in item,item=='sumstats'])]\n",
    "        #string.join(iterable)\n",
    "        print('Procedures are: '+'; '.join(reduced_methods))\n",
    "    \n",
    "    def mc_analysis(self,B=1000,adjust_serial=None,C_FN=30,C_FP=1):\n",
    "       \n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from extra_codes import ndcg_k\n",
    "        import pickle\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        \n",
    "        t0=datetime.now()\n",
    "        # setting the parameters\n",
    "        IS_period=self.ip\n",
    "        k_fold=self.cv_k\n",
    "        OOS_period=self.op # 1 year ahead prediction\n",
    "        OOS_gap=self.og # Gap between training and testing period\n",
    "        start_OOS_year=self.ts[0] #2001\n",
    "        end_OOS_year=self.ts[-1] #2010\n",
    "        sample_start=self.ss #1991\n",
    "        if adjust_serial==None:\n",
    "            adjust_serial=self.a_s\n",
    "        cross_val=self.cv\n",
    "        case_window=self.sa #expanding or rolling window\n",
    "        fraud_df=self.df\n",
    "        write=self.w\n",
    "        \n",
    "        dict_db=pickle.load(open('features_fk.pkl','r+b'))\n",
    "        tbl_ratio_fk=dict_db['lagged_Data']\n",
    "        mapped_X=dict_db['matrix']\n",
    "        red_tbl_fk=tbl_ratio_fk.iloc[:,-46:]\n",
    "        print('pickle file loaded successfully ...')\n",
    "        \n",
    "        tbl_year_IS_CV_index=tbl_ratio_fk[np.logical_and(tbl_ratio_fk.fyear<=2010,\\\n",
    "                                                       tbl_ratio_fk.fyear>=1991)].index\n",
    "        \n",
    "        tbl_year_IS_CV=tbl_ratio_fk.loc[np.logical_and(tbl_ratio_fk.fyear<=2010,\\\n",
    "                                                   tbl_ratio_fk.fyear>=1991)]\n",
    "            \n",
    "        X_CV=mapped_X[tbl_year_IS_CV_index,:]\n",
    "        idx_set=np.where(np.logical_and(np.isnan(X_CV).any(axis=1)==False,\\\n",
    "                                             np.isinf(X_CV).any(axis=1)==False))[0]\n",
    "        tbl_year_IS_CV=tbl_year_IS_CV.iloc[idx_set,:]\n",
    "        \n",
    "        X_CV=X_CV[idx_set,:]\n",
    "        X_CV=(X_CV-np.mean(X_CV,axis=0))/np.std(X_CV,axis=0)\n",
    "        Y_CV=tbl_ratio_fk.AAER_DUMMY[tbl_year_IS_CV_index]\n",
    "        Y_CV=Y_CV.iloc[idx_set]\n",
    "        \n",
    "        P_f=np.sum(Y_CV==1)/len(Y_CV)\n",
    "        P_nf=1-P_f\n",
    "        \n",
    "        print('prior probablity of fraud between '+str(sample_start)+'-'+\n",
    "              str(start_OOS_year-1)+' is '+str(np.round(P_f*100,2))+'%')\n",
    "\n",
    "        # Setting the cross-validation setting\n",
    "        t000=datetime.now() \n",
    "        \n",
    "        range_oos=range(start_OOS_year,end_OOS_year+1,OOS_period) #(2001,2010+1,1)\n",
    "        roc_ratio=np.zeros(len(range_oos))\n",
    "        count_positive_train=np.zeros(len(range_oos))\n",
    "        count_positive_test=np.zeros(len(range_oos))\n",
    "        drop_serial=np.zeros(len(range_oos))\n",
    "        ndcg_ratio=np.zeros(len(range_oos))\n",
    "        sensitivity_ratio=np.zeros(len(range_oos))\n",
    "        specificity_ratio=np.zeros(len(range_oos))\n",
    "        precision_ratio=np.zeros(len(range_oos))\n",
    "        ecm_ratio=np.zeros(len(range_oos))\n",
    "        f1_ratio=np.zeros(len(range_oos))\n",
    "        \n",
    "        m=0\n",
    "        for yr in range_oos: #2001-2010\n",
    "            t1=datetime.now()\n",
    "            if case_window=='expanding':\n",
    "                year_start_IS=sample_start #1991\n",
    "            else:\n",
    "                year_start_IS=yr-IS_period #1991\n",
    "            #how many years between training and testing sample: \n",
    "            #expanding: 1991-2000, 1991-2001\n",
    "            #rolling: 1991-2000, 1992-2001\n",
    "            \n",
    "            reduced_tbl=tbl_ratio_fk[tbl_ratio_fk.fyear>=sample_start] #1991\n",
    "            reduced_tbl=tbl_ratio_fk[tbl_ratio_fk.fyear<=end_OOS_year] #2010\n",
    "            \n",
    "            tbl_year_IS_index=reduced_tbl[np.logical_and(reduced_tbl.fyear<yr-OOS_gap,\\\n",
    "                                                       reduced_tbl.fyear>=year_start_IS)].index\n",
    "        \n",
    "            tbl_year_IS=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear<yr-OOS_gap,\\\n",
    "                                                       reduced_tbl.fyear>=year_start_IS)]\n",
    "            \n",
    "            X=mapped_X[tbl_year_IS_index,:]\n",
    "            idx_set=np.where(np.logical_and(np.isnan(X).any(axis=1)==False,\\\n",
    "                                             np.isinf(X).any(axis=1)==False))[0]\n",
    "            tbl_year_IS=tbl_year_IS.iloc[idx_set,:]\n",
    "        \n",
    "            X=X[idx_set,:]\n",
    "            mean_vals=np.mean(X)\n",
    "            std_vals=np.std(X)\n",
    "            X=(X-mean_vals)/std_vals\n",
    "            Y=tbl_ratio_fk.AAER_DUMMY[tbl_year_IS_index]\n",
    "            Y=Y.iloc[idx_set]  \n",
    "            \n",
    "            misstate_firms=np.unique(tbl_year_IS.gvkey[tbl_year_IS.AAER_DUMMY==1])\n",
    "            #How many periods constitute the testing sample at a time: 2001, 2002\n",
    "            tbl_year_OOS=reduced_tbl.loc[np.logical_and(reduced_tbl.fyear>=yr,\\\n",
    "                                                        reduced_tbl.fyear<yr+OOS_period)]\n",
    "            \n",
    "            if adjust_serial==True:\n",
    "                ok_index=np.zeros(tbl_year_OOS.shape[0])\n",
    "                for s in range(0,tbl_year_OOS.shape[0]):\n",
    "                    if not tbl_year_OOS.iloc[s,1] in misstate_firms:\n",
    "                        ok_index[s]=True\n",
    "                    \n",
    "                \n",
    "            else:\n",
    "                #filled with ones and keep all observations including serial frauds\n",
    "                ok_index=np.ones(tbl_year_OOS.shape[0]).astype(bool)\n",
    "                \n",
    "            #deleting observations where a company appears both in IS and OOS samples\n",
    "            tbl_year_OOS=tbl_year_OOS.iloc[ok_index==True,:]\n",
    "            tbl_year_OOS=tbl_year_OOS.reset_index(drop=True)\n",
    "            tbl_year_OOS_index=tbl_year_OOS.index\n",
    "            \n",
    "            X_OOS=mapped_X[tbl_year_OOS_index,:]\n",
    "            X_OOS=(X_OOS-mean_vals)/std_vals\n",
    "            \n",
    "            Y_OOS=tbl_year_OOS.AAER_DUMMY\n",
    "            n_P=np.sum(Y_OOS==1)\n",
    "            n_N=np.sum(Y_OOS==0)\n",
    "            \n",
    "            scaling = MinMaxScaler(feature_range=(-1,1)).fit(X)\n",
    "            X = scaling.transform(X)\n",
    "            X_OOS = scaling.transform(X_OOS)\n",
    "            \n",
    "            t01=datetime.now()\n",
    "            \n",
    "            print('5')\n",
    "            #require a pickle file containing lagged data of financial ratios\n",
    "            svm_fk=SVC(class_weight={0: 0.02, 1: 1},kernel='linear',shrinking=False,\\\n",
    "                            probability=False,random_state=0,cache_size=1000,\\\n",
    "                                tol=X.shape[-1]*1e-3)\n",
    "            clf_svm_fk=svm_fk.fit(X, Y)\n",
    "            predicted_test=clf_svm_fk.decision_function(X_OOS)\n",
    "            predicted_test[predicted_test>=1]=1+np.log(predicted_test[predicted_test>=1])\n",
    "            predicted_test=np.exp(predicted_test)/(1+np.exp(predicted_test))\n",
    "            roc_ratio[m]=roc_auc_score(Y_OOS,predicted_test)\n",
    "            \n",
    "            #numpy.percentile()function used to compute the nth percentile of the given data (array elements) along the specified axis. \n",
    "            #classification threshold (99th percentile) = 1 or 0\n",
    "            cutoff_ratio=np.percentile(predicted_test,95)\n",
    "            # predicted value higher than the threshold value will flag the observation as positive, whether correctly or not (TP+FP)\n",
    "            labels_ratio=(predicted_test>=cutoff_ratio).astype(int)\n",
    "            #fraud correctly classified \n",
    "            sensitivity_ratio[m]=np.sum(np.logical_and(labels_ratio==1,Y_OOS==1))/np.sum(Y_OOS)\n",
    "            #non-fraud correctly classified\n",
    "            specificity_ratio[m]=np.sum(np.logical_and(labels_ratio==0,Y_OOS==0))/np.sum(Y_OOS==0)\n",
    "            #the number of true positives to the total number of positives \n",
    "            precision_ratio[m]=np.sum(np.logical_and(labels_ratio==1,Y_OOS==1))/np.sum(labels_ratio)\n",
    "            #Pandas Series.to_numpy() function is used to return a NumPy ndarray representing the values in given Series or Index.\n",
    "            ndcg_ratio[m]=ndcg_k(Y_OOS.to_numpy(),predicted_test,95)\n",
    "            \n",
    "            FN=np.sum(np.logical_and(predicted_test<cutoff_ratio, \\\n",
    "                                                          Y_OOS==1))\n",
    "            FP=np.sum(np.logical_and(predicted_test>=cutoff_ratio, \\\n",
    "                                                          Y_OOS==0))\n",
    "            # C_FN: Cost of a False Negative for ECM  -30\n",
    "            #C_FP: Cost of a False Positive for ECM  -1\n",
    "   \n",
    "            ecm_ratio[m]=C_FN*P_f*FN/n_P+C_FP*P_nf*FP/n_N\n",
    "    \n",
    "    \n",
    "            t2=datetime.now() \n",
    "            dt=t2-t1\n",
    "            print('analysis finished for OOS period '+str(yr)+' after '+str(dt.total_seconds())+' sec')\n",
    "            m+=1\n",
    "            \n",
    "        \n",
    "        f1_ratio=2*(precision_ratio*sensitivity_ratio)/(precision_ratio+sensitivity_ratio+1e-8)\n",
    "        \n",
    "        # create performance table now\n",
    "        perf_tbl_general=pd.DataFrame()\n",
    "        perf_tbl_general['models']=['FK23_kfold']\n",
    "        perf_tbl_general['Roc']=[str(np.round(\n",
    "            np.mean(roc_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(roc_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Roc_noise_to_signal']=[str(np.round(\n",
    "            np.std(roc_ratio)/np.mean(roc_ratio)*100,2))+'%']\n",
    "                                                    \n",
    "        perf_tbl_general['Sensitivity @ 1 Prc']=[str(np.round(\n",
    "            np.mean(sensitivity_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(sensitivity_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Sensitivity_noise_to_signal']=[str(np.round(\n",
    "            np.std(sensitivity_ratio)/np.mean(sensitivity_ratio)*100,2))+'%']\n",
    "\n",
    "        perf_tbl_general['Specificity @ 1 Prc']=[str(np.round(\n",
    "            np.mean(specificity_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(specificity_ratio)*100,2))+'%)'] \n",
    "        \n",
    "        perf_tbl_general['Specificity_noise_to_signal']=[str(np.round(\n",
    "            np.std(specificity_ratio)/np.mean(specificity_ratio)*100,2))+'%']\n",
    "        \n",
    "\n",
    "        perf_tbl_general['Precision @ 1 Prc']=[str(np.round(\n",
    "            np.mean(precision_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(precision_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['Precision_noise_to_signal']=[str(np.round(\n",
    "            np.std(precision_ratio)/np.mean(precision_ratio)*100,2))+'%']\n",
    "\n",
    "        perf_tbl_general['F1 Score @ 1 Prc']=[str(np.round(\n",
    "            np.mean(f1_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(f1_ratio)*100,2))+'%)']\n",
    "            \n",
    "        perf_tbl_general['F1 Score_noise_to_signal']=[str(np.round(\n",
    "            np.std(f1_ratio)/np.mean(f1_ratio)*100,2))+'%']\n",
    "        \n",
    "        perf_tbl_general['NDCG @ 1 Prc']=[str(np.round(\n",
    "            np.mean(ndcg_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(ndcg_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['NDCG_noise_to_signal']=[str(np.round(\n",
    "            np.std(ndcg_ratio)/np.mean(ndcg_ratio)*100,2))+'%']\n",
    "        \n",
    "        \n",
    "        perf_tbl_general['ECM @ 1 Prc']=[str(np.round(\n",
    "            np.mean(ecm_ratio)*100,2))+'% ('+\\\n",
    "            str(np.round(np.std(ecm_ratio)*100,2))+'%)']\n",
    "        \n",
    "        perf_tbl_general['ECM_noise_to_signal']=[str(np.round(\n",
    "            np.std(ecm_ratio)/np.mean(ecm_ratio)*100,2))+'%']\n",
    "        \n",
    "        lbl_perf_tbl='MC_results_FK23_kfold'+'.csv'\n",
    "                        \n",
    "        if write==True:\n",
    "            perf_tbl_general.to_csv(lbl_perf_tbl,index=True)\n",
    "        \n",
    "        t001=datetime.now()\n",
    "        dt00=t001-t000\n",
    "        print('MC analysis is completed after '+str(dt00.total_seconds())+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05e97547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module initiated successfully ...\n",
      "Procedures are: mc_analysis()\n",
      "pickle file loaded successfully ...\n",
      "prior probablity of fraud between 1991-2000 is 0.89%\n",
      "5\n",
      "analysis finished for OOS period 2001 after 220.423968 sec\n",
      "5\n",
      "analysis finished for OOS period 2002 after 299.296065 sec\n",
      "5\n",
      "analysis finished for OOS period 2003 after 385.583004 sec\n",
      "5\n",
      "analysis finished for OOS period 2004 after 466.655009 sec\n",
      "5\n",
      "analysis finished for OOS period 2005 after 549.564889 sec\n",
      "5\n",
      "analysis finished for OOS period 2006 after 596.97971 sec\n",
      "5\n",
      "analysis finished for OOS period 2007 after 652.756198 sec\n",
      "5\n",
      "analysis finished for OOS period 2008 after 735.795254 sec\n",
      "5\n",
      "analysis finished for OOS period 2009 after 759.902828 sec\n",
      "5\n",
      "analysis finished for OOS period 2010 after 790.267023 sec\n",
      "MC analysis is completed after 5457.311567 seconds\n"
     ]
    }
   ],
   "source": [
    "a = ML_Fraud(sample_start = 1991,test_sample = range (2001,2011),OOS_per = 1,OOS_gap = 0,sampling = \"expanding\",adjust_serial = True,\n",
    "            cv_flag = False,cv_k = 10,write = True,IS_per = 10)\n",
    "a.mc_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db9f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
